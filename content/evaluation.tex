%!TEX root=../main.tex
\chapter{Discussion and Evaluation}
\label{ch:evaluation}
This chapter will discuss several aspects of this thesis. \Cref{sec:eval-tech} will discuss pros and cons of the real-time updates described in \Cref{sec:real-time}. The Section will also discuss planned immediate next steps which were not integrated into the system before the user test, due to limited time to verify correctness of the code. Furthermore, \Cref{sec:eval-user-testing} presents alternative ways of conducting the user experiment presented in \Cref{ch:testing}, and \Cref{sec:eval-sys-testing} will discuss aspects of the system testing executed as part of this thesis. Finally, we will evaluate in \Cref{sec:eval-pr-achiev} if the goals set in \Cref{sec:ps-inter} have been reached.

\section{Improvements}
\label{sec:eval-tech}

\subsection{Real Time Updates}
\Cref{sec:real-time} describes the implementation of real time updates of data models using Socket.io and WebSockets. However, the current use case of WebSockets in the \gls{cmb} only sends events from the server to the frontend to notify users about submission state updates. Another technology called \gls{sse} \cite{hickson2009} also enables the server to send updates to clients automatically without the need for polling. \gls{sse} uses the HTTP protocol to push updates from the server to connected clients i.e it is not full-duplex as the WebSocket protocol.  \\

The great benefit of \gls{sse} is that it does not introduce a new protocol to achieve real time updates. \gls{sse} is thereby considered more fit to applications which only need to push updates from the server to the connected clients. The downside of \gls{sse} is that it does not support the browser \gls{ie}, which in our case is unacceptable. The user interface of \gls{cmb} is web-based, and we do not want to restrict users to certain browsers or \glspl{os}. Since \gls{ie} is one of the main browsers used world-wide, WebSockets with Socket.io is used instead of \gls{sse}.  \\

A benefit of using Socket.io is that the framework does automatically detect which protocol that is supported by a given client, as mentioned in \Cref{sec:real-time}. As the framework automatically selects the protocol suited for a client, users are not restricted to a specific browser or operating system in order to use the system. As the socket is a full-duplex communication channel, it also makes it possible to implement features which are not possible using \gls{sse}. For the \gls{cmb} system, a online code editor with automatic syntax error highlighting or a messaging service is possible using the Socket.io framework. Appendix \ref{apdx:backlog} and \Cref{sec:future-work} mention possible future extensions using the Socket.io framework. \\

The server uses the modules gevent \cite{GEVENT} and gevent-websocket \cite{GEVENTWEBSOCKET} as mentioned in \Cref{sub-sec:real-time-server}. However, as mentioned on the documentation website of Flask-SocketIO creator Miguel Grinberg, it is also possible to use networking library eventlet \cite{EVENTLET} instead of gevent when using the Flask-SocketIO module \cite{FLASKSOCKETIO}, and is reported to be the best performing option in combination with the module. There are however some benefits of using gevent, as it is tested in real-world high-scale environments and the module interface also follows Python standard library conventions.\footnote{For a further discussion on the matter, see: \url{https://blog.gevent.org/2010/02/27/why-gevent/}, \url{https://groups.google.com/forum/\#!topic/gevent/TelwPl3KgnE}.} The gevent module is therefore used in the \gls{cmb} system.

\subsection{Frontend}
During development, we also planned to enable upload of single and multiple source files. The feature did not have a high priority for the \gls{cmb} team at the start of the thesis, as feedback given by students in TDT4200 indicated that they quickly learned how to submit files to the system using zip-files. However, as indicated by the textual feedback from the user experiment conducted as part of this thesis, the feature is wanted by users, and as a result it has been added to the backlog found in Appendix \ref{apdx:backlog}. \\

If submission timeouts also were added at the server (explained below), the problem-view also had a planned extension of dislaying a progress bar during execution. The progress bar would display the approximate time of execution, but the feature was not implemented as the extension was more complex than what it seemed on first glance. The feature has been added to the backlog in Appendix \ref{apdx:backlog} as a usability improvement.

\subsection{Server}
\label{sec:disc-server}
A couple of server improvements were also considered during development but ended up with lower priority compared to the tasks listed in \Cref{sec:ps-inter}. First, the server should perform a simple check to verify the format of the uploaded zip-file. The frontend currently checks and does simple corrections to the zip file before sending it of to the server, as described in \Cref{sub-sec:impr-frontend-bug}. Since the frontend is the main user interface of the system, zip-files submitted by normal users are therefore checked before sent to the server. However, if the system is to be extended with other user interfaces, for instance a \gls{cli}, it would be beneficial to add server side zip-file checks. \\

During development and maintenance of the system there occurred file-name conflicts when storing submissions in the file system, as submissions are stored by submission name. The situation occured frequently during development of the submission delete endpoint described in \Cref{sub-sec:impr-server-endpoint}. However, to avoid such file-name conflicts in the future, it could be an idea to instead save submission files by the automatically generated database id instead since it's unique. The fix was not implemented due to time limitations before the user test, but is added to the backlog in Appendix \ref{apdx:backlog}. \\

Reporting the run-queue index to users was also planned before the user test. However, the extension turned out to be more complicated than at first glance. The current run-queue\footnote{Uses the Python Queue module: \url{https://docs.python.org/2/library/queue.html}.} is thread-safe and this is also required, as multiple users might access the queue simultaneously. However, there is no way of looking at elements and their index in the currently used queue module without removing them.  \\

A simple solution is to copy the queue and emit its data over Socket.io to each of the connected clients every time a submission is pulled from the queue. If a client has a submission in the queue, the client could then simply loop through the queue and notify users of the new submission index. The solution is probably the simplest to implement, but would possibly impose transportation of unnecessary data to inactive clients. The solution would also increase the amount of network traffic during heavy system load, especially if the system is to be scaled with multiple boards and submissions are rapidly pulled out of the run-queue. As this solution was not discussed with the \gls{cmb} team and there was little time to test the solution before the user experiment, it has not been implemented. The feature can be found in the backlog in Appendix \ref{apdx:backlog}. \\

Timeouts were added to the backend to abort submissions which locked the backend for further use as described in \Cref{sec:impr-backend}. However, submissions could in theory crash from errors currently not handled by the system backend, or might be delayed due to high network traffic. The server should in such cases keep track of timers for each submission, and abort execution of a submission on the backend if a timeout occurs. The server could for instance fork off a gevent coroutine for each submission, and have each coroutine keep track of a timer for a given submission. Upon timeout, the coroutine could then kill the executing program on the backend and update the database with timeout information. \\

This feature was planned prior to the user test but not implemented. First, there was limited time before the user test to implement and test the feature. Second, developing low-level server and backend code was the focus of the Master Thesis written by Christian Chavez. To not interfere with the work done on scalability, the feature was given lower priority in this thesis and has been added to the backlog in Appendix \ref{apdx:backlog}.

\subsection{Backend}
Debugging of submissions running over SSH has in some situations been troublesome as mentioned in \Cref{subsec:related-proj}. The \gls{cmb} team therefore wanted to rewrite the scripts present at the backend into Python scripts instead which makes the scripts easy to unit test. As scalability and code development related to backend functionality were the focus of Master student Christian Chavez, the porting of bash scripts into Python scripts is not considered in this thesis. Only small changes were made to the backend as described in \Cref{sec:impr-backend}, to improve feedback to users in case of submission failures.

\section{User Testing}
\label{sec:eval-user-testing}
The user experiment methodology presented in \Cref{sub-sec:user-testing-methodology} corresponds closely to a static group comparison described by Oates \cite{Oates2006}. The static group comparison divides the participants into two groups, where one of the groups receives some form of treatment (version two of the system) and the other receives no treatment (version one). The effect of the treatment can therefore be assessed by evaluating test scores. There are some downsides with the method, such as in our case, we know that the group testing system version one had used the system longer and also had in interest in parallel C/C++ programming compared to the other group. As noted in \Cref{sub-sec:user-test-validity}, the difference between the two groups might have an affect on the results of the user experiment. \\

Oates also describes other common user experiment setups which could have been used in this thesis. Instead of regarding previous user test results, we could have tested system version one and two on the participants on the user test conducted this Spring only. Participants would then test system version one first and then system version two afterwards, which is known as a one group pre-test and post-test. The usability could then be assessed by comparing pre- and post-test scores. The downside with the method, is that participants might have learned from using system version one and it might affect the results when they are testing system version two. \\

Pre- and post-tests could also have been conducted if we had more participants to the user test conducted in this thesis. Participants would then be split into two random groups, each assessing the usability of system version one. If the randomization has been performed correctly, each group should have as equal assessment of usability of system version one as possible. The test is then run one more time, having one of the groups assessing the usability of system version two instead. The results are then compared, and differences in results of the two assessments are assumed to be caused by the different treatment of the groups. This method also has the same downside as described above: participants might learn from the first round of the user test and use their knowledge when assessing the system a second time. \\

There exists more experiment designs if a lot of participants are registered, such as the Solomon four-group design \cite{Oates2006}. However, as mentioned in \Cref{sub-sec:user-testing-methodology}, there were too few participants to consider using more complex methodologies. The static group comparison was chosen as there were few participants to the second user test, and because the Specialization project already had conducted a usability study. The benefit with the chosen methodology is that a limited number of resources is required. The methodology used only required one server hosting the new system, while the other user testing approaches described in this section requires two; one hosting system version one and another hosting system version two. \\

Most of the questions in the usability questionnaire used were made to a user study conducted during the specialization project. However, a common usability questionnaire like a SUS \cite{brooke1996} could also have been used and may have been easier to analyze and validate. As there were few participants to the user experiment conducted in this thesis, we were forced to use the questionnaire constructed as part of the specialization project to correctly compare the results of the two user tests. But, the questionnaire is as mentioned based on a usability questionnaire developed by IBM and is also inspired by the questionnaire guidelines defined by Oates \cite{Oates2006}. \\

litative analysis should also be put under consideration in the future. Quantitative usability studies mostly measure satisfiability of users, and we cannot verify that users have executed the tasks of the usability test correctly \cite{holzinger2005}. A structured qualitative analysis should be conducted to validate other aspects of usability. However, the continuous user testing conducted, as described in \Cref{sec:cont-user-testing}, partially covers qualitative measures of usability.

\section{System Testing}
\label{sec:eval-sys-testing}
During this thesis several unit tests have been developed and their coverage were presented in \Cref{sec:system-unit-tests}. Also, manual testing has been conducted locally and on the development server of \gls{cmb}. To speed up local manual testing, future developers should consider adding database fixtures\footnote{Database fixtures are defined sets of test data which can be loaded into the database.} to the server code repositories. This would make it easy to load wanted data into the database before manually testing the system. Future developers should also continue to create unit tests and consider adding automatic integration tests to lower the amount of manual testing needed to accept a feature.

\section{Project Objective Achievements}
\label{sec:eval-pr-achiev}
This section will evaluate whether we have reached the objectives defined in \Cref{sec:ps-inter}.

\paragraph*{Main Objectives:} \hfill

\paragraph*{U1 - Fix the main bugs and known issues found during user testing of CMB in November 2015:} Considered covered by \Cref{sub-sec:impr-frontend-bug}. The section described how fixes of Mac OS X uploads (\texttt{U1.1}), locked submissions on backend (\texttt{U1.2}), and the highscore list sorting bug (\texttt{U1.2}) were implemented.

\paragraph*{I1 - Change the existing database management system if necessary:} Considered covered by \Cref{sub-sec:impr-dbms}.The SQLite \gls{dbms} were replaced by the MySQL \gls{dbms}, and all data present in the SQLite databases were transferred to the new MySQL databases for both the development and production server.

\paragraph*{U2 - Improve and extend the CMB system’s usability features in accordance with the CMB team’s priorities:} Considered covered by this thesis. \Cref{sub-sec:impr-views-feedback} described the improvements done to feedback messages reported in the system, as well as upgrades done to the frontend views i.e \texttt{U2.1} and \texttt{U2.5} respectively. The improvement of adding and removing problems through the admin interface is covered by \Cref{sub-sec:impr-admin}. The bulletin board extension to cover goal \texttt{U2.4} was described in \Cref{sub-sec:impr-views-feedback}. \Cref{sec:real-time} described the implementation of real-time model updates with Socket.io (\texttt{U2.2}), which also has a lot of potential for the further development of the system. Some possible extensions are presented in \Cref{sec:future-work} and is also listed in Appendix \ref{apdx:backlog}.

\paragraph*{U3 - Conduct a user-experiment to evaluate system usability:} Considered covered by \Cref{sec:user-testing}. A user test was conducted to evaluate system usability with focus on efficiency, learnability and satisfiability. As the user experiment used much the same questionnaire used in the user study conducted during the specialization project, a statistical analysis was conducted to compare the possibly improved system to the system developed by Follan and Støa.

\paragraph*{Secondary Objectives:} \hfill

\paragraph*{P1 - Propose improvements to the existing stability test to simulating users and their submissions:} Considered covered by \Cref{sub-sec:prop-stability-test}. The Section described two possible Python modules which could be used to extend the stability test developed during the Specialization project. The proposed improvement have been added to the backlog found in Appendix \ref{apdx:backlog}.

\paragraph*{P2 - Propose how to improve the how-to information, the problems offered by \gls{cmb}, and add new problems:} Considered partially covered by this thesis. The improvements to the how-to information were discussed in \Cref{sub-sec:prop-howto}, and the proposed improvements have been added to the backlog in Appendix \ref{apdx:backlog}. This thesis has not contributed towards adding new problems, but \Cref{sub-sec:prop-problems} proposed that newly added problems use the same format as other \glspl{oj}.

\paragraph*{P3 - Propose how to implement a discussion forum:} Considered covered by \Cref{sub-sec:prop-forum}. The section proposed various third-party forum software packages which can be used by the system. Also, the section proposed that future developers should consider developing a discussion forum from scratch, to make the system look more professional.

\paragraph*{I2 Implement some of the proposed solutions after approval by, and in collaboration with the CMB team:} Not considered covered by this thesis. All proposals are currently listed in the backlog found in Appendix \ref{apdx:backlog}.
