%!TEX root=../main.tex
\chapter{Background}
\label{ch:background}

\section{The Mont Blanc Project}
The Mont Blanc project \cite{m:MB} started in October 2011. The Barcelona Supercomputing Center coordinates the project. It received 14 million Euros as initial funding, where the European Commission granted 8 of those millions. Two years after the project started, it received another 8 million Euros from the European Commission with a total of 22 million Euros of funding. This budget is enough to make the project last until September 2016. As mentioned in section \ref{sec:mot}, the long-term goal is to reach Exascale performance using 15 to 30 times less energy than other HPC systems. The energy performance metric used is FLOPS/W, motivated by the Green500 list. Ramirez reported during PRACEdays'14 \cite{m:MB-PRACE-14} that Mont-Blanc part 1, valid from 2011 to 2015, had three main objectives. The first was to deploy a prototype scalable to 50 PFLOPS using a total of 7MW using the available energy efficient embedded technology that should be competitive with the Green500 leaders of 2014. The second objective was to overcome some limitations found during development of the prototype system and design the next generation HPC system. It should be scalable 200 PFLOPS on 10 MW, which should be competitive with the Top500 leaders in 2017. The third and last objective was to port Exascale applications capable of exploiting the new generation of HPC systems from goal 2. Ramirez also reported the goals of Mont Blanc part 2, valid from 2013 to 2016. The first objective is to supplement the Mont Blanc system software stack. The second goal is to construct the initial definition of the Exascale architecture. Objective number three are to be up to date with newly released ARM products and evaluate if they are suitable for HPC architectures. The last and fourth objective is continued support for the Mont Blanc project. These objectives is already summarized by Follan and Støa \cite{mt:T&S} and on the project website \cite{m:MB}.\\*

% Ramirez reported during PRACEdays'14 \cite{m:MB-PRACE-14} that Mont-Blanc part 1, valid from 2011 to 2015, had three main objectives. The first was to deploy a prototype scalable to 50 PFLOPS using a total of 7MW using the available energy efficient embedded technology that should be competitive with the Green500 leaders of 2014. The second objective was to overcome some limitations found during development of the prototype system and design the next generation HPC system. It should be scalable 200 PFLOPS on 10 MW, which should be competitive with the Top500 leaders in 2017. The third and last objective was to port Exascale applications capable of exploiting the new generation of HPC systems from goal 2. Ramirez also reported the goals of Mont Blanc part 2, valid from 2013 to 2016. The first objective is to supplement the Mont Blanc system software stack. The second goal is to construct the initial definition of the Exascale architecture. Objective number three are to be up to date with newly released ARM products and evaluate if they are suitable for HPC architectures. The last and fourth objective is continued support for the Mont Blanc project.

The current status of the project was reported by Mantovani \cite{m:MB-15}. Among the tasks finished so far is the deployment of a software stack to support the Mont Blanc system (see prototype below), porting HPC kernels and applications and the design, development, deployment and monitoring the prototypes. Amidst the ongoing work is ARM 64 bit exploration, porting new applications for the HPC architecture developed, enhance the programming model (OmpSs \cite{m:ompss}) used and monitor the HPC prototype for fault tolerant techniques. \\*

\noindent
The project has already generated multiple prototypes, where the ones with known specs are listed below in the order of development:
\begin{enumerate}
\item \textbf{Tibidabo} \cite{a:MB:Tib}: Tibidabo is the first prototype of the MB project. The prototype was made to show that it is possible to deploy a large scale HPC cluster using ARM processors. It is mainly an experimental cluster containing compute boards with NVIDIA Tegra 2 SoC, with dual core ARM Cortex A9 @ 1GHz. The GPU's on the chip did not support the standard programming models OpenCL and CUDA and was therefore not used in the cluster. However, the compute cards can be replaced with up-to-date chips as they become available with updated GPU's that support the two programming models. The prototype consists of 128 nodes, each containing a Q7 module. Each Q7 module contains a Tegra 2 chip. The prototype achieves 120 MFLOPS/W on High-Performance Linpack (HPL) benchmarks.
\item \textbf{Pedraforca} \cite{m:MB:Pedr}: Pedraforca consist of combination of ARM processors and NVIDIA GPU's. One compute node consists of a NVIDIA K20 GPU and a Tegra 3 Q7 Module containing 4 ARM Cortex A9 @ 1.3 GHz. The system contains 78 such compute nodes. Some factors in the technology used limits the system, but it contributes to the project by using of ARM-based processors in embedded systems and to explore limitations with this technology. It is the first large HPC system that makes use of ARM-based processors.
\item \textbf{Mont Blanc} \cite{m:MB:MB-prot}: The latest prototype from the MB project (updated specs in 2015\cite{m:MB-15}). The system contains a rack with a total of 1080 Exynos 5 compute cards, or  2160 ARM Cortex-A15@1.7 GHz CPUs, 1080 ARM Mali T604 GPUs, 4.3 TB of RAM and 17.2 TB of Flash memory. The performance is about 35 TFLOPS and the power consumption is about 24 kW. In November 2014, the Mont Blanc Prototype had an energy efficiency of 1.5 GFLOPS/W. The best Green500 ranking during that time was approximately 5.2 GFLOPS/W.
\end{enumerate}


\section{The Climbing Mont Blanc Project}
Climbing Mont Blanc (CMB) is an Online Judge focusing on energy efficient programming on ARM based platforms, greatly inspired by the Mont Blanc project. An Online Judge is a web application or webpage which let it's users upload programming solutions to problems provided by the Online Judge. The paper "\textit{Climbing Mont Blanc - A Training Site for Energy Efficient Programming on Heterogeneous Multicore Processors}" written by Lasse Natvig et.al \cite{a:CMB} describes the CMB system. Appendix \ref{appendix/paper} includes the whole paper. The first version of CMB was developed by Torbjørn Folland and Simen Støa during their Master thesis during the Spring of 2015 \cite{mt:T&S}. The source code is available on \textit{Bitbucket} \cite{m:bitbucket}, which uses \textit{git} \cite{m:git} version control. The architecture of CMB is shown in figure \ref{fig:cmb_arch}. As seen by the figure, the system consists of three main parts; the \textit{frontend}, the \textit{server} and the \textit{backend}.

\subsection{Architecture}
\subsubsection{Frontend} The user interface . It handles user based interactions like logging in and out of the system, view problems, submit solutions to problems, compile and run submissions, view high score lists and join/create groups. The frontend is written in Javascript (ECMAScript $5^{th}$ Edition) and AngularJS \cite{m:angular}. AngularJS is a framework developed by Google and are now maintained by Google and individual developers. The framework is built according to the Model View Controller pattern and extends HTML with dynamic views with two-way data binding for building single page applications. The result is a smoother user experience, as the page updates the views dynamically with no need of refreshing the web-page. The framework also lets the developer define own reusable components, which can make the code base more structured. The view structure is defined in HTML5 while CSS3 takes care of the view styling. Google Analytics monitors the frontend, which is an advanced monitoring software provided by Google. There is a vast number of things that can be monitored and analyzed. Among the things monitored is the number of users, number of new users and user behavior. Chapter 5 of Torbjørn and Simen's Master Thesis  contains more information about system monitoring, and a detailed description of the technologies used \cite{mt:T&S}. \\

The system can be reached at \url{https://climb.idi.ntnu.no}. In this paragraph, \textit{navigate} means to perform an action to change the view in the user interface. The home page of CMB is shown in figure \ref{fig:front-page}. From here, the user can login, sign up, create and join groups, view the \textit{HowTo}-page, view problems and navigate to a specific problem to solve. If the user navigates to a specific problem, the view differs depending on the state of the user. If the user is not logged in, it may only view the submitted programs that are set to be \textit{visible}\footnote{A user may change the visibility of their submissions and results through the user interface.}. The user may upload programs if they are signed-in, and if so the problem view is changed to make it possible to upload files. The file uploaded currently needs to be a zip file containing the source code. Further usage information of the system are found in Appendix \ref{appendix/paper} and the Master Thesis of Follan and Støa. \\

\subsubsection{Server} is implemented as an REpresentional State Transfer (REST) Application Programming Interface (API), as defined in Roy T. Fielding doctoral dissertation \cite{phd:rtf}. The RESTful API is implemented using Python Flask \cite{m:flask}. The server also makes use of Gunicorn \cite{m:guni} to handle simultaneous requests from multiple users. On our development and production servers, Nginx \cite{m:nginx} is also used as a \textit{reverse proxy}. It serves static files on the fly while requests for dynamic content (updated submissions etc.) are forwarded to Gunicorn. SQLite \cite{m:sqlite} is used as database with SQLAlchemy\cite{m:sqlalc} on top. SQLAlchemy functions as an Object Relational Mapper (ORM) that is it associates Python classes with database tables and objects of those classes with rows in the tables. SQLAlchemy makes it easy to interact with the database, as one only modify object structures. The server runs within a Python \textit{Virtual environment} \cite{m:virtualenv}, which have all Python required dependencies\footnote{Dependencies is meant by dependencies in between packages (read: code libraries). As an example, SQLAlchemy might require a specific version of Python Flask installed in order to work correctly.} installed within a virtual environment. The virtual environment contains only the packages needed by the server, which removes potential dependency errors if other Python based systems also runs on the server. The server is responsible for handling requests from the client, storing useful information in the database (like submissions) and compiling submitted programs. A Python program called \textit{push.py} is run in the background, constantly checking a run queue for runnable programs. If there is a program ready to run in the queue, the script will push the program over to the backend, and then compile and run it using SSH. The result is then sent to the server, and the server checks the correctness of the program before reporting the result back to the frontend. Finally, the server is responsible for checking the state of the system with a background cron job. The cron job runs every 15 minutes, checking if the three parts Gunicorn, \textit{push.py} and backend is up and running. The system sends out an automatic e-mail in case of system failure.

\subsubsection{Admin Interface} Some users have special privileges and are marked \textit{admins} in the database, which grants them access to the admin interface found at \url{http://climb.idi.ntnu.no/admin}. The admins can control a lot of the content visible on the frontend. One such procedure is to add a new problem. In short, the admin needs to register a new problem and upload all files needed to the server.  Procedures that modify the database or the system in any way are all done through the admin interface. The files to include when making a problem is the input files targeted for the submitted programs, expected answer files and a special file called \textit{checker.cpp} that checks the correctness of the submitted program. The checker program is ran on the output of the program, and comparing the answer to the expected result given the input to the program. A special database field called \textit{goodness} can also be set by the program checker. The field was added for approximation problems to check for how "good" a solution is. As an example, a solution to the Vertex Cover problem might yield a different cover on each run. This way, the admin making the problem can specify if a cover size is good or not.

\subsubsection{Backend} . It is an Odroid XU3 board \cite{m:odroid} which consists of an Exynos 5 Octa heterogeneous multicore. A block diagram of the board is shown in figure \ref{fig:odroid-block}. CMB only uses a minimal of the external interfaces and components on the system: the Ethernet port for internet access, eMMC module or MicroSD card for the OS and file system and the board energy monitors. The Exynos 5 Octa consists of four big ARM Cortex A-15 and four small ARM Cortex A-7 cores. Both CPU types have the same Instruction Set Architecture (ISA), but they have different characteristics when it comes to performance and power consumption. The Exynos chip also has an ARM Mali-T628 GPU with six cores. With the two ARM processor types and the Mali GPU, the chip can be called a \textit{three way heterogenous multicore with 14 cores}. The cores also use the ARM big.LITTLE technology, which makes it possible for the Operating System (OS) to perform Global Task Scheduling, that is to dynamically assign threads to the most appropriate CPU based on the run-time information \cite{m:big-little}. The board will compile and run the programs pushed over to it by \textit{push.py}. While running uploaded programs, the board will measure time and energy (see subsection \ref{sec:em-cmb}). When the board finishes running and calculating energy consumption for a given program, it will send the result back to the server for further processing. The observant reader might view a compilation both at the server and at the board as redundant. First, there is a lack of good enough cross-compiler. Second, the compilation at the server gives quicker feedback to the user in case of compilation errors. The CMB system currently supports the programming languages C and C++, compiled with gcc-4.9 and g++-4.9 respectively. The compilers have support for both OpenCL v1.1, OpenMP 4.0 and PThreads NTPL 2.19. CMB will be extended to support other programming languages and libraries (see section \ref{sec:future} for more information). Section 5 of the CMB paper, found in Appendix \ref{appendix/paper}, explains the planned extensions to CMB.

\subsection{Energy Measurements}
\label{sec:em-cmb}
The Hardkernel EnergyMonitor program that is compatible with the Odroid XU3 is used to monitor the energy consumption of the submitted programs. The pipeline in figure \ref{fig:pipeline} presents the execution pipeline for the backend on submission of a program. When a program is to run, it is first compiled and then run with small input to check for runtime errors. If there are no runtime errors, the EnergyMonitor program is started. The cache is then cleared and the CPU is heated using the UNIX \textit{stress} \cite{m:stress} command. Clearing the cache is done due to irregularities in energy readings which were a bug discovered during the Spring 2014. The CPU runs \textit{stress} to obtain a stable target temperature (currently 60 degrees Celsius) before running the program as it might also have an effect on energy readings, as proposed by Juan M. Cebrian and Lasse Natvig \cite{a:JL:T}. For a discussion of this matter, consult Follan and Støa's Master Thesis \cite{mt:T&S}. The current backend uses the sensors monitoring the temperature of the four big ARM Cortex A15 cores to arrive at the given target temperature of 60 degrees. The program is then run, and finally the EnergyMonitor program stops and the result is reported back to the server as seen in the figure. \\

Right before and right after a program has run, timestamps measure the time spent to execute the whole program. As seen by figure \ref{fig:pipeline}, the EnergyMonitor program is started before the running of the submitted program. To ensure accurate energy readings, the timestamps used to measure running time is also used to fetch the correct energy measurements from the output of the EnergyMonitor program.  As mentioned, when the program finishes, the monitoring stops and the energy consumption is calculated and returned to the server. The server then calculates the Energy Delay Product (EDP), which is the main energy metric used in CMB, as it emphasizes both energy and performance \cite{b:edp}. Energy is in itself not a good energy metric, since we could simply run a program on a slow and under clocked CPU to obtain low energy consumption. EDP is defined in equation \ref{eq:edp}, where \textit{E} is the energy used, \textit{D} is the delay or execution time.

\begin{equation}
  \label{eq:edp}
  EDP = E * D
\end{equation}

By comparison, the Mont Blanc project uses the metric FLOPS/W. This metric may be a good metric when running the same benchmark (that is the same implementation) over and over again to test a system architecture. However, CMB compares different implementations of known problems. Because of these limitations it is difficult to use the FLOPS/W metric in the CMB system, as the number of operations may differ from one implementation to the next. Another problem is to determine the number of \textit{useful} instructions, that is instructions that contribute towards solving the problem. For these reasons, it is much easier to use EDP as energy metric.

\subsection{Code Correctness and System Environment}
To quickly deploy new versions of the code, the system makes use of \textit{Jenkins} \cite{m:jenkins} as Continuous Integration (CI) server or build server. CI is a software engineering practice that automates the testing and deployment of systems. Some benefits of CI is reported by Fowler in his article from 2006; a clone of the production environment (so-called development server) runs tests on the code change before deployment to production, bugs are discovered and removed easily, and as a whole it makes it possible with rapid integration of new features \cite{a:F:CI}. The CMB development server has the a Jenkins server installed. It is easier to describe what CI and Jenkins are with a description with a example from the CMB system as below. Jenkins makes it easy to define \textit{build pipelines}. Each build pipeline consists of \textit{jobs} and \textit{triggers}. A job can among many things pull code from a Bitbucket repository, execute scripts, execute scripts over SSH, or \textit{trigger} other jobs. An example build pipeline is shown in figure \ref{fig:server-ci}, which shows the build pipeline for the server. The execution of the pipeline starts with sensing a change in the Bitbucket repository and pulling the newly changed code. The job then installs all Python requirements with \textit{pip} \cite{m:pip}, which is the Python package manager (within the virtual environment as explained above). It then triggers the next job, which is the running of \textit{automatic unit tests} with \textit{nose} \cite{m:nose} and \textit{linting}\footnote{Linting is the automatic process to uncover errors and to ensure some coding standards.} with \textit{flake8} \cite{m:flake}. If the testing and linting passes, the job then triggers the next. The next job automatically deploys and set up the test server for further manual testing by executing scripts over SSH. The code is run in with debugging enabled, such that logs are available when doing manual tests. One can trigger the next job manually, which deploys the change to the to the production server. The deployment to production involves both setting up the frontend and backend for production and restarting the server. The frontend code is set up with minimized and concatenated HTML, CSS and Javascript files to improve performance. \\

The frontend build pipeline has a similar procedure. The only difference is minor changes in the commands run by Jenkins. The first job pulls code from the Bitbucket frontend repository if a change has happened, and install all requirements with the package manager \textit{npm} \cite{m:npm}. It then runs the frontend code linter, \textit{jshint} \cite{m:jshint} for Javascript code, and sets up the frontend in debug mode (developer mode) through a tool called \textit{gulp} \cite{m:gulp}. Gulp is a build system that enables the programmer to define \textit{tasks} that may consist of multiple commands to achieve some goals. The goals of this step are first to run the task of running the linter and then run the task of building the source code with debugging enabled. If the Jenkins job is successful, the next job in the pipeline is triggered. The second step makes use of another package manager called \textit{bower} \cite{m:bower}, which installs dependencies used in the unit tests for the frontend. The Jenkins job then executes the automatic tests with gulp. If the automatic unit tests are successful, a third Jenkins job is triggered. The job simply deploys to the CMB development server, by pulling all changes, installing all requirements with \textit{npm}, and finally setup the frontend in debug mode through a task in gulp. To deploy to the production environment, one can manually trigger the Jenkins job like on the server pipeline. The job deploys the code change to the CMB production server, much like the Jenkins job for deploying to production. The only change is that it only sets up the frontend, it does not do a full server restart like above. The figure \ref{fig:cmb-server} summarizes the server environment and the interactions between each of the CMB servers with Jenkins. The observant reader might see that the workflow is the same for both pipelines. First Jenkins setup the code, then it runs automatic unit tests, then deploys to the development server, and lastly may manually deploy to the production server. When a change happens in the Bitbucket repository (some developer have developed new code) launches Jenkins and the workflow is executed. All development should be done locally, it is considered bad practice to develop new features on either the development or production server.

\subsection{Security}
\paragraph*{User Information Security}
User specific information is stored in the database. Among the information stored is the user name and password of all users. The password itself is ran through a method called \textit{generate\_password\_hash} \cite{m:gph}, which is part of the Python \textit{Werkzeug security} package, upon user creation. This ensures that the password is hashed before stored in the database. The admin users also have similar hashed passwords stored in their table in the database. The same password will generate the same hash every time, but it is not possible to reconstruct the password from the hashed password found in the database. The password is sent and checked upon login, and if the password hash matches the hash in the database, a \textit{token}\footnote{A token is some unique string valid for a specific period of time} is sent back to the user. This token contains enough information to uniquely describe a user session. The token is sent in the HTTP Authorization Request Header, which makes it possible for the server to return user specific information. The token is valid for one hour, which means that a user can be inactive up to one hour. After the token expires, the user needs to log back in to access user specific information. The token is stored at the frontend, which makes the server stateless. To make the transfer of information more secure, the HTTPS protocol is used to encrypt all information sent in a request. HTTPS makes use of Secure Socket Layer (SSL) encryption, which makes it almost impossible to decrypt messages sent from the server. For example, fetching a user name or password during log in or user creation will be hard to achieve.

\paragraph*{Uploaded Programs Security}
Programs uploaded can potentially contain malicious code or file names. It is hard to predict if the uploaded code is malicious or not, and it is easier to restrict the permissions of the user running the uploaded program. The solution is to let a user with as few permissions as possible run the uploaded program. The CMB backend therefore has a user called \textit{worker} for that purpose. To secure and remove potential threats residing in the file name, the Python Werkzeug security procedure \textit{secure\_filename} \cite{m:sf} is used before storing the uploaded file. The system also uses a predefined Makefile to compile the uploaded programs. If an unknown error occurs, it might crash the server. One way of making the server more secure and stable in such cases, is to have a separate compile server or thread. If the compilation server crashes unexpected, the system still stays active to handle more requests. This has not been implemented, since no such problem has occurred.

\paragraph*{System Security}
As mentioned, to secure the messages sent, HTTPS is used which encrypts and decrypts messages with Secure Socket Layer (SSL). The reverse proxy used at the servers, Nginx, is configured to handle HTTPS requests. SSL requires a SSL Certificate to work, which is a guarantee that the holder of the certificate is trustworthy. Most browsers has a list of Certificate Authorities (CA), which is some third party issuing the certificates. Uninett created the certificate that is used by the CMB system which is known by most browsers. The messages sent from CMB is therefore trusted. The backend of CMB also has Uncomplicated Firewall (UFW) \cite{m:ufw} installed, which restricts access to the board. The firewall requires every SSH requests to come from within the NTNU network itself, which means that the backend is only accessible from NTNU or through a Virtual Private Network (VPN). The backend also has Fail2ban  \cite{m:f2b} installed, which restricts the number of authentication attempts made within a certain time-frame. If there are more than three attempts within the configured time-frame, the IP sending the requests is banned for 10 minutes. The server also has UFW and Fail2ban installed, but the setup of UFW is slightly different. UFW on the server let's all requests to port 80 and 443 be open for all requests, to enable the use of HTTP and HTTPS respectively. This makes sure that the CMB frontend is accessible from outside the NTNU network. To keep up to date with security updates \textit{unattended-updates} \cite{m:uau} is enabled, which installs security updates if needed. This is done at 02:00, which also automatically restarts the CMB system.

\subsection{Planned extensions}
Extensions to be made this spring and in the future.
Simon Holbacka++ for accurate energy readings.
